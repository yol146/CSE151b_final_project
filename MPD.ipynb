{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718c38cf",
   "metadata": {},
   "source": [
    "## Install the package dependencies before running this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ac7530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "# Get the device\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b472cf2",
   "metadata": {},
   "source": [
    "## Create a Torch.Dataset class for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "091abbb7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./argo2/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# # intialize a dataset\n",
    "city = 'austin' \n",
    "split = 'train'\n",
    "\n",
    "def normalize(trajectories):\n",
    "    minxy = np.min(trajectories, axis=-1)\n",
    "    maxxy = np.max(trajectories, axis=-1)\n",
    "    scale = np.max(maxxy - minxy) / 2.0\n",
    "    return (trajectories / scale), scale\n",
    "\n",
    "def unnormalize(trajectories, scale):\n",
    "    return trajectories * scale\n",
    "\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "#         tensor = torch.Tensor(self.feature[item])\n",
    "#         tensorlabel = torch.Tensor(self.label[item])\n",
    "#         return tensor.to(device), tensorlabel.to(device)\n",
    "\n",
    "        data = np.concatenate((self.inputs[idx], self.outputs[idx]))\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.Tensor(data).to(device)\n",
    "    \n",
    "\n",
    "# process data\n",
    "inputs, outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "all_in_1 = []\n",
    "for i in range(len(inputs)):\n",
    "    togetherData = np.concatenate((inputs[i], outputs[i]))\n",
    "    all_in_1.append(togetherData)\n",
    "\n",
    "\n",
    "\n",
    "# every sentence has length 50, with the 51st character as target value\n",
    "# set range step higher if overfit     \n",
    "# if dataArr is with velocity, remove for y labels\n",
    "def create_set(dataArr, normalization=True):\n",
    "    normArr = dataArr\n",
    "    scale = 0\n",
    "    set_x = []\n",
    "    set_y = []\n",
    "    if(normalization):\n",
    "        normArr, scale = normalize(dataArr)\n",
    "    for i, elem in enumerate(normArr):\n",
    "        for j in range(0, 60):\n",
    "            set_x.append(elem[j:j+50])\n",
    "            set_y.append(elem[j+50])\n",
    "    return set_x, set_y, scale\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.feature = x\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        tensor = torch.Tensor(self.feature[item])\n",
    "        tensorlabel = torch.Tensor(self.label[item])\n",
    "        return tensor.to(device), tensorlabel.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "# input: n*110*2 paths\n",
    "# output: n*110*4 paths with dim2 = [x,y,Vx,Vy], where Vx, Vy are velocity with length/.1s as unit\n",
    "def addVelocity(dataArr):\n",
    "    dataWV = []\n",
    "    for path in np.array(dataArr):\n",
    "        pathWV = []\n",
    "        pathWV.append(np.concatenate((path[0], path[1] - path[0])))\n",
    "        for coord in range(1, len(path)):\n",
    "            vel = path[coord] - path[coord-1]\n",
    "            coordWV = np.concatenate((path[coord], vel))\n",
    "            pathWV.append(coordWV)\n",
    "        dataWV.append(pathWV)\n",
    "    return dataWV\n",
    "            \n",
    "\n",
    "# tempNorm, tempScale = normalize(all_in_1)\n",
    "# tempunNorm = unnormalize(tempNorm, tempScale)\n",
    "# print (\"tempNorm\", tempNorm)\n",
    "# print (\"unNorm\", tempunNorm)\n",
    "# print (\"original\", np.array(all_in_1))\n",
    "\n",
    "\n",
    "    \n",
    "all_in_1_WV = addVelocity(all_in_1)\n",
    "training_set_x, training_set_y, scale = create_set(all_in_1_WV, normalization=False)\n",
    "train_dataset = mydataset(training_set_x, training_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d30f35",
   "metadata": {},
   "source": [
    "Current Velocity is not normalized, which adds weight to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfcbf419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (np.array(training_set_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d0d4011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 4)\n"
     ]
    }
   ],
   "source": [
    "inputs, outputs = get_city_trajectories(city=city, split='val', normalized=False)\n",
    "val_in_1 = []\n",
    "for i in range(len(inputs)):\n",
    "    togetherData = np.concatenate((inputs[i], outputs[i]))\n",
    "    val_in_1.append(togetherData)\n",
    "\n",
    "# every sentence has length 40, with the 41st character as target value\n",
    "# set range step higher if overfit\n",
    "\n",
    "# random subsampling from val_set to make faster validation\n",
    "numValPath = 500\n",
    "val_in_1_WV = addVelocity(val_in_1)\n",
    "val_index = np.random.choice(len(val_in_1_WV), numValPath, replace=False)\n",
    "val_set_x, val_set_y, scale_val = create_set(np.array(val_in_1_WV)[val_index])\n",
    "val_dataset = mydataset(val_set_x, val_set_y)        \n",
    "print (np.array(val_set_y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a174510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = ArgoverseDataset(city = city, split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a44d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(val_dataset)\n",
    "# print(len(val_dataset))\n",
    "\n",
    "\n",
    "# print (np.array(all_in_1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed583e1",
   "metadata": {},
   "source": [
    "Changed batch size from 64 to 60 -> a complete path\n",
    "Used normalization to get rid of distrubacnes of coordiantes\n",
    "Considering: angle, velocity only as features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058453cc",
   "metadata": {},
   "source": [
    "## Create a DataLoader class for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c14f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 5  # batch size \n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_sz, shuffle = True)\n",
    "test_loader = DataLoader(val_dataset, batch_size=batch_sz, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee5b7ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        #Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        #imput dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        \n",
    "        #Building your LSTM\n",
    "        #batch_first=True causes input/output tensors to be of shape\n",
    "        #(batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        #Readout layer\n",
    "        self.fc = nn.Linear (hidden_dim, output_dim)\n",
    "        \n",
    "#         self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "#         print(\"h0 dtype: \", h0.dtype)\n",
    "        \n",
    "        #Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        \n",
    "        #Output channel\n",
    "#         output, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        output, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        hn = hn.view(-1,self.hidden_dim)\n",
    "        out = self.fc(hn)\n",
    "        \n",
    "#         out = output.view(-1,self.hidden_dim)\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         output, state = self.lstm(x, prev_state)\n",
    "#         output = output.type(torch.LongTensor)\n",
    "#         output = output.to(device)\n",
    "#         out = self.Softmax(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "#     def init_state(self, sequence_length):\n",
    "#         return (torch.zeros(self.layer_dim, sequence_length, self.input_dim),\n",
    "#                 torch.zeros(self.layer_dim, sequence_length, self.input_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "140be148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 \n",
      " LSTMModel(\n",
      "  (lstm): LSTM(4, 150, batch_first=True)\n",
      "  (fc): Linear(in_features=150, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# epochs\n",
    "epochs = 20\n",
    "input_dim = 4\n",
    "hidden_dim = 150\n",
    "layer_count = 1\n",
    "output_dim = 4\n",
    "\n",
    "lstm = LSTMModel(input_dim, hidden_dim, layer_count, output_dim).to(device)\n",
    "print(device, \"\\n\", lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adbc1d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = Pred()\n",
    "# opt = optim.Adam(lstm.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e98b6af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0 torch.Size([600, 4])\n",
      "lstm.weight_hh_l0 torch.Size([600, 150])\n",
      "lstm.bias_ih_l0 torch.Size([600])\n",
      "lstm.bias_hh_l0 torch.Size([600])\n",
      "fc.weight torch.Size([2, 150])\n",
      "fc.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "learning_rate = 0.0005\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# visualize the inner data shape\n",
    "for name, param in lstm.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e3625622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "lossArr = []\n",
    "# tranining function\n",
    "def train(dataloader, model, patience=3):\n",
    "#     enable gradients\n",
    "    model.train()\n",
    "    print(\"Begin time: \", time.ctime())\n",
    "    last_loss = 0\n",
    "    patience= patience\n",
    "    for epoch in range(epochs):\n",
    "#         sh, sc = model.init_state(sequence_length)\n",
    "        totalLoss = 0\n",
    "        for batch , (x,y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "    #         print(\"shape of x: \", x.shape)\n",
    "            pred = model(x)\n",
    "#             print(pred)\n",
    "            trainLoss = loss(pred, y)\n",
    "            totalLoss += trainLoss.item()\n",
    "#             print(trainLoss)\n",
    "#             sh = sh.detach()\n",
    "#             sc = sc.detach()\n",
    "            trainLoss.backward()\n",
    "            optimizer.step()\n",
    "            if (batch % 20000 == 0):\n",
    "                print(\"Time: \", time.ctime() , \"batch: \", batch, \"loss: \", totalLoss)\n",
    "        lossArr.append(totalLoss)\n",
    "        print(\"Time: \", time.ctime() , \"epoch: \", epoch, \"loss: \", totalLoss)\n",
    "        #        early stopping\n",
    "#         TODO: change this into validation set! make a smaller validation set\n",
    "        if (last_loss < totalLoss and last_loss != 0):\n",
    "            print(\"EarlyStopping remain patience: \", patience)\n",
    "            if (patience <= 0):\n",
    "                print (\"Early Stopped\")\n",
    "                return\n",
    "            patience -= 1\n",
    "        last_loss = totalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3b3e982",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin time:  Thu May 26 03:27:58 2022\n",
      "Time:  Thu May 26 03:27:58 2022 batch:  0 loss:  1.5564373825327493e-05\n",
      "Time:  Thu May 26 03:29:18 2022 batch:  20000 loss:  1.1493562495045317\n",
      "Time:  Thu May 26 03:30:35 2022 batch:  40000 loss:  2.0584551931853956\n",
      "Time:  Thu May 26 03:31:45 2022 batch:  60000 loss:  2.7875986017015046\n",
      "Time:  Thu May 26 03:32:57 2022 batch:  80000 loss:  3.4683670297109783\n",
      "Time:  Thu May 26 03:34:11 2022 batch:  100000 loss:  4.024495722311246\n",
      "Time:  Thu May 26 03:35:32 2022 batch:  120000 loss:  4.514120538220255\n",
      "Time:  Thu May 26 03:36:44 2022 batch:  140000 loss:  4.99233716255585\n",
      "Time:  Thu May 26 03:37:59 2022 batch:  160000 loss:  5.3967822891218615\n",
      "Time:  Thu May 26 03:39:13 2022 batch:  180000 loss:  5.78959033090773\n",
      "Time:  Thu May 26 03:40:23 2022 batch:  200000 loss:  6.128169597496248\n",
      "Time:  Thu May 26 03:41:38 2022 batch:  220000 loss:  6.498278759922766\n",
      "Time:  Thu May 26 03:42:53 2022 batch:  240000 loss:  6.898848493038593\n",
      "Time:  Thu May 26 03:44:09 2022 batch:  260000 loss:  7.321401919926991\n",
      "Time:  Thu May 26 03:45:19 2022 batch:  280000 loss:  7.661091285425785\n",
      "Time:  Thu May 26 03:46:32 2022 batch:  300000 loss:  8.007710849155451\n",
      "Time:  Thu May 26 03:47:43 2022 batch:  320000 loss:  8.353639160100926\n",
      "Time:  Thu May 26 03:48:53 2022 batch:  340000 loss:  8.664436718506119\n",
      "Time:  Thu May 26 03:50:05 2022 batch:  360000 loss:  8.989071145169119\n",
      "Time:  Thu May 26 03:51:17 2022 batch:  380000 loss:  9.29987727674683\n",
      "Time:  Thu May 26 03:52:29 2022 batch:  400000 loss:  9.579139199137419\n",
      "Time:  Thu May 26 03:53:15 2022 epoch:  0 loss:  9.79549087036408\n",
      "Time:  Thu May 26 03:53:15 2022 batch:  0 loss:  1.6049689293140545e-06\n",
      "Time:  Thu May 26 03:54:27 2022 batch:  20000 loss:  0.2551460012130786\n",
      "Time:  Thu May 26 03:55:37 2022 batch:  40000 loss:  0.529851200369304\n",
      "Time:  Thu May 26 03:56:49 2022 batch:  60000 loss:  0.8117971168394318\n",
      "Time:  Thu May 26 03:58:01 2022 batch:  80000 loss:  1.0746334417028047\n",
      "Time:  Thu May 26 03:59:12 2022 batch:  100000 loss:  1.3211799798211574\n",
      "Time:  Thu May 26 04:00:23 2022 batch:  120000 loss:  1.5579628107758452\n",
      "Time:  Thu May 26 04:01:35 2022 batch:  140000 loss:  1.8333110257400107\n",
      "Time:  Thu May 26 04:02:47 2022 batch:  160000 loss:  2.034866925482719\n",
      "Time:  Thu May 26 04:03:57 2022 batch:  180000 loss:  2.265273933598018\n",
      "Time:  Thu May 26 04:05:09 2022 batch:  200000 loss:  2.485423887935236\n",
      "Time:  Thu May 26 04:06:21 2022 batch:  220000 loss:  2.689120491155712\n",
      "Time:  Thu May 26 04:07:31 2022 batch:  240000 loss:  2.9309915457408593\n",
      "Time:  Thu May 26 04:08:44 2022 batch:  260000 loss:  3.1802605451832315\n",
      "Time:  Thu May 26 04:09:56 2022 batch:  280000 loss:  3.3552128641958903\n",
      "Time:  Thu May 26 04:11:08 2022 batch:  300000 loss:  3.6209836753686444\n",
      "Time:  Thu May 26 04:12:17 2022 batch:  320000 loss:  3.810179856974653\n",
      "Time:  Thu May 26 04:13:30 2022 batch:  340000 loss:  4.001807122646814\n",
      "Time:  Thu May 26 04:14:43 2022 batch:  360000 loss:  4.205740564586745\n",
      "Time:  Thu May 26 04:15:53 2022 batch:  380000 loss:  4.407642382630921\n",
      "Time:  Thu May 26 04:17:05 2022 batch:  400000 loss:  4.595201146011569\n",
      "Time:  Thu May 26 04:17:53 2022 epoch:  1 loss:  4.724049490689042\n",
      "Time:  Thu May 26 04:17:53 2022 batch:  0 loss:  6.143932751001557e-06\n",
      "Time:  Thu May 26 04:19:02 2022 batch:  20000 loss:  0.16255237732671124\n",
      "Time:  Thu May 26 04:20:15 2022 batch:  40000 loss:  0.3436744082839288\n",
      "Time:  Thu May 26 04:21:27 2022 batch:  60000 loss:  0.5429459298466426\n",
      "Time:  Thu May 26 04:22:36 2022 batch:  80000 loss:  0.7124318976509997\n",
      "Time:  Thu May 26 04:23:48 2022 batch:  100000 loss:  0.8790547689446155\n",
      "Time:  Thu May 26 04:25:01 2022 batch:  120000 loss:  1.0552138630527323\n",
      "Time:  Thu May 26 04:26:10 2022 batch:  140000 loss:  1.2565410297640034\n",
      "Time:  Thu May 26 04:27:22 2022 batch:  160000 loss:  1.3933564488065597\n",
      "Time:  Thu May 26 04:28:35 2022 batch:  180000 loss:  1.5551244917994016\n",
      "Time:  Thu May 26 04:29:46 2022 batch:  200000 loss:  1.7182221934709825\n",
      "Time:  Thu May 26 04:30:56 2022 batch:  220000 loss:  1.8611706275352082\n",
      "Time:  Thu May 26 04:32:08 2022 batch:  240000 loss:  2.0462560027770578\n",
      "Time:  Thu May 26 04:33:20 2022 batch:  260000 loss:  2.2399331892306225\n",
      "Time:  Thu May 26 04:34:30 2022 batch:  280000 loss:  2.36423984533734\n",
      "Time:  Thu May 26 04:35:42 2022 batch:  300000 loss:  2.537681355543228\n",
      "Time:  Thu May 26 04:36:54 2022 batch:  320000 loss:  2.680288354846235\n",
      "Time:  Thu May 26 04:38:05 2022 batch:  340000 loss:  2.8265510892266867\n",
      "Time:  Thu May 26 04:39:15 2022 batch:  360000 loss:  2.980047818993019\n",
      "Time:  Thu May 26 04:40:26 2022 batch:  380000 loss:  3.1366995276439287\n",
      "Time:  Thu May 26 04:41:37 2022 batch:  400000 loss:  3.2835824519351915\n",
      "Time:  Thu May 26 04:42:22 2022 epoch:  2 loss:  3.378283349393786\n",
      "Time:  Thu May 26 04:42:22 2022 batch:  0 loss:  9.149924153462052e-06\n",
      "Time:  Thu May 26 04:43:38 2022 batch:  20000 loss:  0.1290657245201336\n",
      "Time:  Thu May 26 04:44:52 2022 batch:  40000 loss:  0.2713583879818118\n",
      "Time:  Thu May 26 04:46:06 2022 batch:  60000 loss:  0.43757840266664844\n",
      "Time:  Thu May 26 04:47:20 2022 batch:  80000 loss:  0.5537084641096736\n",
      "Time:  Thu May 26 04:48:32 2022 batch:  100000 loss:  0.6865461632771745\n",
      "Time:  Thu May 26 04:49:48 2022 batch:  120000 loss:  0.8328049957374178\n",
      "Time:  Thu May 26 04:51:00 2022 batch:  140000 loss:  0.9955145596875152\n",
      "Time:  Thu May 26 04:52:12 2022 batch:  160000 loss:  1.1059923358112627\n",
      "Time:  Thu May 26 04:53:22 2022 batch:  180000 loss:  1.2346316389686185\n",
      "Time:  Thu May 26 04:54:34 2022 batch:  200000 loss:  1.3684682916730135\n",
      "Time:  Thu May 26 04:55:46 2022 batch:  220000 loss:  1.4806874958941003\n",
      "Time:  Thu May 26 04:56:58 2022 batch:  240000 loss:  1.6377967563112716\n",
      "Time:  Thu May 26 04:58:08 2022 batch:  260000 loss:  1.7984467228649679\n",
      "Time:  Thu May 26 04:59:22 2022 batch:  280000 loss:  1.8956879243389284\n",
      "Time:  Thu May 26 05:00:34 2022 batch:  300000 loss:  2.04389043053925\n",
      "Time:  Thu May 26 05:01:43 2022 batch:  320000 loss:  2.15822427441312\n",
      "Time:  Thu May 26 05:02:55 2022 batch:  340000 loss:  2.278887218638757\n",
      "Time:  Thu May 26 05:04:07 2022 batch:  360000 loss:  2.405685883527257\n",
      "Time:  Thu May 26 05:05:19 2022 batch:  380000 loss:  2.5350827902334974\n",
      "Time:  Thu May 26 05:06:31 2022 batch:  400000 loss:  2.6525811750812887\n",
      "Time:  Thu May 26 05:07:19 2022 epoch:  3 loss:  2.742594300725147\n",
      "Time:  Thu May 26 05:07:19 2022 batch:  0 loss:  1.3608226254291367e-05\n",
      "Time:  Thu May 26 05:08:29 2022 batch:  20000 loss:  0.1062943668412581\n",
      "Time:  Thu May 26 05:09:41 2022 batch:  40000 loss:  0.28743994996992117\n",
      "Time:  Thu May 26 05:10:53 2022 batch:  60000 loss:  0.4255107691940979\n",
      "Time:  Thu May 26 05:12:03 2022 batch:  80000 loss:  0.5199725704484042\n",
      "Time:  Thu May 26 05:13:14 2022 batch:  100000 loss:  0.6316369869063523\n",
      "Time:  Thu May 26 05:14:27 2022 batch:  120000 loss:  0.7632640483126175\n",
      "Time:  Thu May 26 05:15:39 2022 batch:  140000 loss:  0.8971422592656622\n",
      "Time:  Thu May 26 05:16:48 2022 batch:  160000 loss:  0.9908940602485966\n",
      "Time:  Thu May 26 05:18:01 2022 batch:  180000 loss:  1.0987973543972749\n",
      "Time:  Thu May 26 05:19:12 2022 batch:  200000 loss:  1.2130390668137105\n",
      "Time:  Thu May 26 05:20:22 2022 batch:  220000 loss:  1.3213628386468634\n",
      "Time:  Thu May 26 05:21:34 2022 batch:  240000 loss:  1.4457296417778582\n",
      "Time:  Thu May 26 05:22:46 2022 batch:  260000 loss:  1.5780168493517301\n",
      "Time:  Thu May 26 05:23:58 2022 batch:  280000 loss:  1.659257195937238\n",
      "Time:  Thu May 26 05:25:08 2022 batch:  300000 loss:  1.7876021247352716\n",
      "Time:  Thu May 26 05:26:20 2022 batch:  320000 loss:  1.8883404185505737\n",
      "Time:  Thu May 26 05:27:33 2022 batch:  340000 loss:  2.0126317988202547\n",
      "Time:  Thu May 26 05:28:42 2022 batch:  360000 loss:  2.115700037487219\n",
      "Time:  Thu May 26 05:29:55 2022 batch:  380000 loss:  2.2511251683809745\n",
      "Time:  Thu May 26 05:31:07 2022 batch:  400000 loss:  2.356537771894684\n",
      "Time:  Thu May 26 05:31:53 2022 epoch:  4 loss:  2.422106793536247\n",
      "Time:  Thu May 26 05:31:53 2022 batch:  0 loss:  4.7830849325691815e-06\n",
      "Time:  Thu May 26 05:33:05 2022 batch:  20000 loss:  0.09101191494802047\n",
      "Time:  Thu May 26 05:34:17 2022 batch:  40000 loss:  0.18969864714417015\n",
      "Time:  Thu May 26 05:35:27 2022 batch:  60000 loss:  0.31160335397837885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  Thu May 26 05:36:39 2022 batch:  80000 loss:  0.39157667307837213\n",
      "Time:  Thu May 26 05:37:51 2022 batch:  100000 loss:  0.4794238616413253\n",
      "Time:  Thu May 26 05:39:01 2022 batch:  120000 loss:  0.5844354866367878\n",
      "Time:  Thu May 26 05:40:13 2022 batch:  140000 loss:  0.6966609847687016\n",
      "Time:  Thu May 26 05:41:24 2022 batch:  160000 loss:  0.7807206084456664\n",
      "Time:  Thu May 26 05:42:35 2022 batch:  180000 loss:  0.8680885499255833\n",
      "Time:  Thu May 26 05:43:44 2022 batch:  200000 loss:  0.9658847479656218\n",
      "Time:  Thu May 26 05:44:56 2022 batch:  220000 loss:  1.0527206985797906\n",
      "Time:  Thu May 26 05:46:10 2022 batch:  240000 loss:  1.1609333884794693\n",
      "Time:  Thu May 26 05:47:23 2022 batch:  260000 loss:  1.2695164035694835\n",
      "Time:  Thu May 26 05:48:39 2022 batch:  280000 loss:  1.3376308502129906\n",
      "Time:  Thu May 26 05:49:54 2022 batch:  300000 loss:  1.4341312092918324\n",
      "Time:  Thu May 26 05:51:05 2022 batch:  320000 loss:  1.5252875663385248\n",
      "Time:  Thu May 26 05:52:15 2022 batch:  340000 loss:  1.6119225081388242\n",
      "Time:  Thu May 26 05:53:27 2022 batch:  360000 loss:  1.7064368158506151\n",
      "Time:  Thu May 26 05:54:40 2022 batch:  380000 loss:  1.800335726344932\n",
      "Time:  Thu May 26 05:55:50 2022 batch:  400000 loss:  1.8926972295982516\n",
      "Time:  Thu May 26 05:56:38 2022 epoch:  5 loss:  1.9514706567052846\n",
      "Time:  Thu May 26 05:56:38 2022 batch:  0 loss:  3.991989160567755e-06\n",
      "Time:  Thu May 26 05:57:50 2022 batch:  20000 loss:  0.08124218906906061\n",
      "Time:  Thu May 26 05:59:00 2022 batch:  40000 loss:  0.16928202077082924\n",
      "Time:  Thu May 26 06:00:13 2022 batch:  60000 loss:  0.2794382638604164\n",
      "Time:  Thu May 26 06:01:25 2022 batch:  80000 loss:  0.3492427914198457\n",
      "Time:  Thu May 26 06:02:34 2022 batch:  100000 loss:  0.43154800990194897\n",
      "Time:  Thu May 26 06:03:46 2022 batch:  120000 loss:  0.5309519436284166\n",
      "Time:  Thu May 26 06:04:59 2022 batch:  140000 loss:  0.6634477858444137\n",
      "Time:  Thu May 26 06:06:09 2022 batch:  160000 loss:  0.7522455127927448\n",
      "Time:  Thu May 26 06:07:21 2022 batch:  180000 loss:  0.8373239062923802\n",
      "Time:  Thu May 26 06:08:33 2022 batch:  200000 loss:  0.9268229574383665\n",
      "Time:  Thu May 26 06:09:45 2022 batch:  220000 loss:  1.0057132678164031\n",
      "Time:  Thu May 26 06:10:55 2022 batch:  240000 loss:  1.1022013363035423\n",
      "Time:  Thu May 26 06:12:08 2022 batch:  260000 loss:  1.2110696061542425\n",
      "Time:  Thu May 26 06:13:20 2022 batch:  280000 loss:  1.2934383263795861\n",
      "Time:  Thu May 26 06:14:30 2022 batch:  300000 loss:  1.3941941030042653\n",
      "Time:  Thu May 26 06:15:42 2022 batch:  320000 loss:  1.4770210726160988\n",
      "Time:  Thu May 26 06:16:54 2022 batch:  340000 loss:  1.5819859711732511\n",
      "Time:  Thu May 26 06:18:06 2022 batch:  360000 loss:  1.6696906651207888\n",
      "Time:  Thu May 26 06:19:17 2022 batch:  380000 loss:  1.7602491139976726\n",
      "Time:  Thu May 26 06:20:30 2022 batch:  400000 loss:  1.8677969969668666\n",
      "Time:  Thu May 26 06:21:15 2022 epoch:  6 loss:  1.9222094553994464\n",
      "Time:  Thu May 26 06:21:15 2022 batch:  0 loss:  3.5091093195660505e-06\n",
      "Time:  Thu May 26 06:22:27 2022 batch:  20000 loss:  0.074236393740748\n",
      "Time:  Thu May 26 06:23:39 2022 batch:  40000 loss:  0.15697600137254167\n",
      "Time:  Thu May 26 06:24:51 2022 batch:  60000 loss:  0.25705454924563853\n",
      "Time:  Thu May 26 06:26:01 2022 batch:  80000 loss:  0.3200357944570589\n",
      "Time:  Thu May 26 06:27:13 2022 batch:  100000 loss:  0.39516044508678116\n",
      "Time:  Thu May 26 06:28:26 2022 batch:  120000 loss:  0.501543538414432\n",
      "Time:  Thu May 26 06:29:36 2022 batch:  140000 loss:  0.6748494023920073\n",
      "Time:  Thu May 26 06:30:48 2022 batch:  160000 loss:  0.7446868300191208\n",
      "Time:  Thu May 26 06:32:00 2022 batch:  180000 loss:  0.8229743422052375\n",
      "Time:  Thu May 26 06:33:09 2022 batch:  200000 loss:  0.9556553744643856\n",
      "Time:  Thu May 26 06:34:22 2022 batch:  220000 loss:  1.0273323348929788\n",
      "Time:  Thu May 26 06:35:34 2022 batch:  240000 loss:  1.5109255770464944\n",
      "Time:  Thu May 26 06:36:47 2022 batch:  260000 loss:  1.6150023705562322\n",
      "Time:  Thu May 26 06:37:57 2022 batch:  280000 loss:  1.6743056326105523\n",
      "Time:  Thu May 26 06:39:09 2022 batch:  300000 loss:  1.7583180918533927\n",
      "Time:  Thu May 26 06:40:21 2022 batch:  320000 loss:  1.8419506185800607\n",
      "Time:  Thu May 26 06:41:31 2022 batch:  340000 loss:  1.9235416091459496\n",
      "Time:  Thu May 26 06:42:43 2022 batch:  360000 loss:  2.009042768888776\n",
      "Time:  Thu May 26 06:43:54 2022 batch:  380000 loss:  2.0950034827082824\n",
      "Time:  Thu May 26 06:45:05 2022 batch:  400000 loss:  2.1747800636852337\n",
      "Time:  Thu May 26 06:45:52 2022 epoch:  7 loss:  2.2224067985989144\n",
      "EarlyStopping remain patience:  3\n",
      "Time:  Thu May 26 06:45:52 2022 batch:  0 loss:  8.155075192917138e-06\n",
      "Time:  Thu May 26 06:47:03 2022 batch:  20000 loss:  0.07221784962518905\n",
      "Time:  Thu May 26 06:48:14 2022 batch:  40000 loss:  0.1506776634549251\n",
      "Time:  Thu May 26 06:49:30 2022 batch:  60000 loss:  0.24312527562114708\n",
      "Time:  Thu May 26 06:50:45 2022 batch:  80000 loss:  0.30276356713776553\n",
      "Time:  Thu May 26 06:52:00 2022 batch:  100000 loss:  0.3737947771448263\n",
      "Time:  Thu May 26 06:53:10 2022 batch:  120000 loss:  0.4768578738528766\n",
      "Time:  Thu May 26 06:54:23 2022 batch:  140000 loss:  0.693066020027623\n",
      "Time:  Thu May 26 06:55:35 2022 batch:  160000 loss:  0.7722169022925433\n",
      "Time:  Thu May 26 06:56:45 2022 batch:  180000 loss:  0.8472402685391713\n",
      "Time:  Thu May 26 06:57:57 2022 batch:  200000 loss:  0.9273089607603823\n",
      "Time:  Thu May 26 06:59:09 2022 batch:  220000 loss:  1.0013820573980898\n",
      "Time:  Thu May 26 07:00:21 2022 batch:  240000 loss:  1.1069426416982955\n",
      "Time:  Thu May 26 07:01:33 2022 batch:  260000 loss:  1.2034739304935562\n",
      "Time:  Thu May 26 07:02:45 2022 batch:  280000 loss:  1.2585007355859978\n",
      "Time:  Thu May 26 07:03:57 2022 batch:  300000 loss:  1.3427284867334837\n",
      "Time:  Thu May 26 07:05:06 2022 batch:  320000 loss:  1.4224562357555859\n",
      "Time:  Thu May 26 07:06:19 2022 batch:  340000 loss:  1.4960545545528674\n",
      "Time:  Thu May 26 07:07:31 2022 batch:  360000 loss:  1.5789493223345377\n",
      "Time:  Thu May 26 07:08:41 2022 batch:  380000 loss:  1.6595566446818126\n",
      "Time:  Thu May 26 07:09:53 2022 batch:  400000 loss:  1.741759505332195\n",
      "Time:  Thu May 26 07:10:41 2022 epoch:  8 loss:  1.783353290534961\n",
      "Time:  Thu May 26 07:10:41 2022 batch:  0 loss:  7.221446139737964e-06\n",
      "Time:  Thu May 26 07:11:51 2022 batch:  20000 loss:  0.06615611229876038\n",
      "Time:  Thu May 26 07:13:04 2022 batch:  40000 loss:  0.13966888197797053\n",
      "Time:  Thu May 26 07:14:16 2022 batch:  60000 loss:  0.23001150733420406\n",
      "Time:  Thu May 26 07:15:26 2022 batch:  80000 loss:  0.2862575945074634\n",
      "Time:  Thu May 26 07:16:37 2022 batch:  100000 loss:  0.3530674320387999\n",
      "Time:  Thu May 26 07:17:50 2022 batch:  120000 loss:  0.45481078019116467\n",
      "Time:  Thu May 26 07:19:01 2022 batch:  140000 loss:  0.6214016414134912\n",
      "Time:  Thu May 26 07:20:12 2022 batch:  160000 loss:  0.6927168157388334\n",
      "Time:  Thu May 26 07:21:24 2022 batch:  180000 loss:  0.7609600250258945\n",
      "Time:  Thu May 26 07:22:36 2022 batch:  200000 loss:  0.8343516307461947\n",
      "Time:  Thu May 26 07:23:45 2022 batch:  220000 loss:  0.902586267597636\n",
      "Time:  Thu May 26 07:24:58 2022 batch:  240000 loss:  0.9917265145051488\n",
      "Time:  Thu May 26 07:26:10 2022 batch:  260000 loss:  1.0900157610395467\n",
      "Time:  Thu May 26 07:27:20 2022 batch:  280000 loss:  1.136519556648307\n",
      "Time:  Thu May 26 07:28:32 2022 batch:  300000 loss:  1.199112853717865\n",
      "Time:  Thu May 26 07:29:44 2022 batch:  320000 loss:  1.2701355195389181\n",
      "Time:  Thu May 26 07:30:56 2022 batch:  340000 loss:  1.3437999686078053\n",
      "Time:  Thu May 26 07:32:06 2022 batch:  360000 loss:  1.4492195817822529\n",
      "Time:  Thu May 26 07:33:18 2022 batch:  380000 loss:  1.5248099289278019\n",
      "Time:  Thu May 26 07:34:30 2022 batch:  400000 loss:  1.6110853148019677\n",
      "Time:  Thu May 26 07:35:16 2022 epoch:  9 loss:  6.899817199558058\n",
      "EarlyStopping remain patience:  2\n",
      "Time:  Thu May 26 07:35:16 2022 batch:  0 loss:  2.249376620966359e-06\n",
      "Time:  Thu May 26 07:36:29 2022 batch:  20000 loss:  0.051293804135881166\n",
      "Time:  Thu May 26 07:37:41 2022 batch:  40000 loss:  0.12676429426999675\n",
      "Time:  Thu May 26 07:38:51 2022 batch:  60000 loss:  0.2004702532629702\n",
      "Time:  Thu May 26 07:40:03 2022 batch:  80000 loss:  0.25693978286966923\n",
      "Time:  Thu May 26 07:41:15 2022 batch:  100000 loss:  1.6226902322032855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  Thu May 26 07:42:24 2022 batch:  120000 loss:  4.011900441206608\n",
      "Time:  Thu May 26 07:43:37 2022 batch:  140000 loss:  4.121223814983946\n",
      "Time:  Thu May 26 07:44:49 2022 batch:  160000 loss:  4.194912864439308\n",
      "Time:  Thu May 26 07:46:00 2022 batch:  180000 loss:  4.2663843810401545\n",
      "Time:  Thu May 26 07:47:09 2022 batch:  200000 loss:  4.358726080854454\n",
      "Time:  Thu May 26 07:48:20 2022 batch:  220000 loss:  4.42183580977112\n",
      "Time:  Thu May 26 07:49:32 2022 batch:  240000 loss:  4.505119081675181\n",
      "Time:  Thu May 26 07:50:43 2022 batch:  260000 loss:  4.590705910361145\n",
      "Time:  Thu May 26 07:51:58 2022 batch:  280000 loss:  4.643297848885921\n",
      "Time:  Thu May 26 07:53:14 2022 batch:  300000 loss:  4.7130981869825606\n",
      "Time:  Thu May 26 07:54:27 2022 batch:  320000 loss:  4.799700204107966\n",
      "Time:  Thu May 26 07:55:39 2022 batch:  340000 loss:  4.862883128072113\n",
      "Time:  Thu May 26 07:56:51 2022 batch:  360000 loss:  4.945360780350709\n",
      "Time:  Thu May 26 07:58:03 2022 batch:  380000 loss:  5.00442872290165\n",
      "Time:  Thu May 26 07:59:13 2022 batch:  400000 loss:  5.07383626579456\n",
      "Time:  Thu May 26 08:00:01 2022 epoch:  10 loss:  5.114299890392031\n",
      "Time:  Thu May 26 08:00:01 2022 batch:  0 loss:  6.7182950260757934e-06\n",
      "Time:  Thu May 26 08:01:10 2022 batch:  20000 loss:  0.053630682626303716\n",
      "Time:  Thu May 26 08:02:23 2022 batch:  40000 loss:  0.12115409534790593\n",
      "Time:  Thu May 26 08:03:35 2022 batch:  60000 loss:  0.18743061408494838\n",
      "Time:  Thu May 26 08:04:47 2022 batch:  80000 loss:  0.24575976935124788\n",
      "Time:  Thu May 26 08:05:56 2022 batch:  100000 loss:  0.3050687979080569\n",
      "Time:  Thu May 26 08:07:09 2022 batch:  120000 loss:  0.399644688933036\n",
      "Time:  Thu May 26 08:08:21 2022 batch:  140000 loss:  0.47772281895657265\n",
      "Time:  Thu May 26 08:09:31 2022 batch:  160000 loss:  0.5373932416105023\n",
      "Time:  Thu May 26 08:10:43 2022 batch:  180000 loss:  0.6005383608502168\n",
      "Time:  Thu May 26 08:11:55 2022 batch:  200000 loss:  0.663930763197175\n",
      "Time:  Thu May 26 08:13:06 2022 batch:  220000 loss:  0.7289037307647472\n",
      "Time:  Thu May 26 08:14:17 2022 batch:  240000 loss:  0.7931486992357655\n",
      "Time:  Thu May 26 08:15:29 2022 batch:  260000 loss:  0.8692053364245419\n",
      "Time:  Thu May 26 08:16:41 2022 batch:  280000 loss:  0.9133846600409722\n",
      "Time:  Thu May 26 08:17:51 2022 batch:  300000 loss:  0.9732757970811146\n",
      "Time:  Thu May 26 08:19:03 2022 batch:  320000 loss:  1.0479713429679558\n",
      "Time:  Thu May 26 08:20:15 2022 batch:  340000 loss:  1.1068941624778228\n",
      "Time:  Thu May 26 08:21:25 2022 batch:  360000 loss:  1.1784114391624936\n",
      "Time:  Thu May 26 08:22:37 2022 batch:  380000 loss:  1.2439924615240645\n",
      "Time:  Thu May 26 08:23:50 2022 batch:  400000 loss:  1.3138433228335047\n",
      "Time:  Thu May 26 08:24:35 2022 epoch:  11 loss:  1.3494499192200122\n",
      "Time:  Thu May 26 08:24:35 2022 batch:  0 loss:  1.3232343007985037e-06\n",
      "Time:  Thu May 26 08:25:47 2022 batch:  20000 loss:  0.06259505497678183\n",
      "Time:  Thu May 26 08:27:00 2022 batch:  40000 loss:  0.12741675864022994\n",
      "Time:  Thu May 26 08:28:09 2022 batch:  60000 loss:  0.1949276806965008\n",
      "Time:  Thu May 26 08:29:22 2022 batch:  80000 loss:  0.24325482598961196\n",
      "Time:  Thu May 26 08:30:34 2022 batch:  100000 loss:  0.30058180952331637\n",
      "Time:  Thu May 26 08:31:46 2022 batch:  120000 loss:  0.38408164626933483\n",
      "Time:  Thu May 26 08:32:56 2022 batch:  140000 loss:  0.461133474977171\n",
      "Time:  Thu May 26 08:34:09 2022 batch:  160000 loss:  0.5159428498999257\n",
      "Time:  Thu May 26 08:35:21 2022 batch:  180000 loss:  0.5766255367701032\n",
      "Time:  Thu May 26 08:36:31 2022 batch:  200000 loss:  0.6392928272240546\n",
      "Time:  Thu May 26 08:37:43 2022 batch:  220000 loss:  0.6973851173407939\n",
      "Time:  Thu May 26 08:38:56 2022 batch:  240000 loss:  0.7622020827155174\n",
      "Time:  Thu May 26 08:40:08 2022 batch:  260000 loss:  0.8294142909561515\n",
      "Time:  Thu May 26 08:41:18 2022 batch:  280000 loss:  0.8795247660316409\n",
      "Time:  Thu May 26 08:42:30 2022 batch:  300000 loss:  0.9487292075834077\n",
      "Time:  Thu May 26 08:43:42 2022 batch:  320000 loss:  1.027244889040745\n",
      "Time:  Thu May 26 08:44:52 2022 batch:  340000 loss:  1.0808176026042515\n",
      "Time:  Thu May 26 08:46:05 2022 batch:  360000 loss:  1.4238942947994921\n",
      "Time:  Thu May 26 08:47:17 2022 batch:  380000 loss:  1.4908134372274215\n",
      "Time:  Thu May 26 08:48:27 2022 batch:  400000 loss:  1.5557841523360314\n",
      "Time:  Thu May 26 08:49:15 2022 epoch:  12 loss:  1.5920291484115794\n",
      "EarlyStopping remain patience:  1\n",
      "Time:  Thu May 26 08:49:15 2022 batch:  0 loss:  2.236993623228045e-06\n",
      "Time:  Thu May 26 08:50:26 2022 batch:  20000 loss:  0.05770230971622516\n",
      "Time:  Thu May 26 08:51:36 2022 batch:  40000 loss:  0.11835789641666514\n",
      "Time:  Thu May 26 08:52:47 2022 batch:  60000 loss:  0.21183206399623078\n",
      "Time:  Thu May 26 08:54:02 2022 batch:  80000 loss:  0.2595708737930165\n",
      "Time:  Thu May 26 08:55:16 2022 batch:  100000 loss:  0.32090292072957816\n",
      "Time:  Thu May 26 08:56:31 2022 batch:  120000 loss:  0.429419555371387\n",
      "Time:  Thu May 26 08:57:46 2022 batch:  140000 loss:  0.5619674363355459\n",
      "Time:  Thu May 26 08:58:59 2022 batch:  160000 loss:  0.6276573059383392\n",
      "Time:  Thu May 26 09:00:10 2022 batch:  180000 loss:  0.6862595382839928\n",
      "Time:  Thu May 26 09:01:23 2022 batch:  200000 loss:  0.7486830963682877\n",
      "Time:  Thu May 26 09:02:35 2022 batch:  220000 loss:  0.802445288269645\n",
      "Time:  Thu May 26 09:03:45 2022 batch:  240000 loss:  0.8662169240707156\n",
      "Time:  Thu May 26 09:04:58 2022 batch:  260000 loss:  0.9290032357095303\n",
      "Time:  Thu May 26 09:06:11 2022 batch:  280000 loss:  0.9819827392498427\n",
      "Time:  Thu May 26 09:07:24 2022 batch:  300000 loss:  1.049725639815616\n",
      "Time:  Thu May 26 09:08:34 2022 batch:  320000 loss:  1.1126146825762135\n",
      "Time:  Thu May 26 09:09:46 2022 batch:  340000 loss:  1.167058787275212\n",
      "Time:  Thu May 26 09:10:59 2022 batch:  360000 loss:  1.2315918749713992\n",
      "Time:  Thu May 26 09:12:09 2022 batch:  380000 loss:  1.2935023100635608\n",
      "Time:  Thu May 26 09:13:22 2022 batch:  400000 loss:  1.3519462389295807\n",
      "Time:  Thu May 26 09:14:11 2022 epoch:  13 loss:  1.3896569005262949\n",
      "Time:  Thu May 26 09:14:11 2022 batch:  0 loss:  3.744406740224804e-06\n",
      "Time:  Thu May 26 09:15:21 2022 batch:  20000 loss:  0.05594122010309468\n",
      "Time:  Thu May 26 09:16:34 2022 batch:  40000 loss:  0.1138904848234979\n",
      "Time:  Thu May 26 09:17:47 2022 batch:  60000 loss:  0.36255109201351415\n",
      "Time:  Thu May 26 09:18:57 2022 batch:  80000 loss:  0.4117599962010118\n",
      "Time:  Thu May 26 09:20:09 2022 batch:  100000 loss:  0.4717273555191843\n",
      "Time:  Thu May 26 09:21:22 2022 batch:  120000 loss:  0.5449881426037562\n",
      "Time:  Thu May 26 09:22:34 2022 batch:  140000 loss:  0.609967768843676\n",
      "Time:  Thu May 26 09:23:45 2022 batch:  160000 loss:  0.6610996523510319\n",
      "Time:  Thu May 26 09:24:58 2022 batch:  180000 loss:  0.7174530827806408\n",
      "Time:  Thu May 26 09:26:10 2022 batch:  200000 loss:  0.7755411866996595\n",
      "Time:  Thu May 26 09:27:20 2022 batch:  220000 loss:  0.8287142017704902\n",
      "Time:  Thu May 26 09:28:32 2022 batch:  240000 loss:  0.889823182611681\n",
      "Time:  Thu May 26 09:29:45 2022 batch:  260000 loss:  0.9563912871490501\n",
      "Time:  Thu May 26 09:30:56 2022 batch:  280000 loss:  0.9998656962209853\n",
      "Time:  Thu May 26 09:32:08 2022 batch:  300000 loss:  1.0660385300732453\n",
      "Time:  Thu May 26 09:33:21 2022 batch:  320000 loss:  1.1296694129443368\n",
      "Time:  Thu May 26 09:34:33 2022 batch:  340000 loss:  1.181640338676236\n",
      "Time:  Thu May 26 09:35:43 2022 batch:  360000 loss:  1.2441410390635101\n",
      "Time:  Thu May 26 09:36:55 2022 batch:  380000 loss:  1.3048104688410151\n",
      "Time:  Thu May 26 09:38:08 2022 batch:  400000 loss:  1.3604339071891727\n",
      "Time:  Thu May 26 09:38:54 2022 epoch:  14 loss:  1.3956399594924738\n",
      "EarlyStopping remain patience:  0\n",
      "Early Stopped\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0dd26999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the trained model\n",
    "# torch.save(lstm.state_dict(), \"trained_lstm\" + \"_lr%s_epoch%d_hd%d\" % (str(learning_rate), epochs, hidden_dim))\n",
    "# numpy.save(\"./lossArr/\" + \"trained_lstm\" + \"_lr%s_epoch%d_hd%d\" % (str(learning_rate), epochs, hidden_dim), lossArr)\n",
    "# load the trained model\n",
    "# lstm.load_state_dict(torch.load(\"trained_lstm_lr0.0005_epoch50_hd150\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5b0d0969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lossArr)\n",
    "plt.title('Loss Graph')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "949a1af9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.000002 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict(seed, model):\n",
    "    model.eval()\n",
    "    prepseed = torch.Tensor(seed).to(device)\n",
    "    res = model(seed.to(device))\n",
    "    res = res.cpu().detach().numpy()\n",
    "#     print (res.shape)\n",
    "    return res\n",
    "\n",
    "def test(dataloader, model):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    testLoss, correct = 0,0\n",
    "    with torch.no_grad():\n",
    "        for batch, (x,y) in enumerate(dataloader):\n",
    "            pred = model(x)\n",
    "            testLoss += loss(pred, y).item()\n",
    "    testLoss /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {testLoss:>8f} \\n\")\n",
    "\n",
    "test(test_loader, lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e83f62e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.541725 -8.866736]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # gives a 50 timestep data and generates the future 60 timesteps\n",
    "# # def genPath(seed, model):\n",
    "# #     model.eval()\n",
    "    \n",
    "# #     resAll = seed\n",
    "# #     for i in range(60):\n",
    "# #         resAll = np.append (resAll, predict(torch.unsqueeze(torch.FloatTensor(resAll[i:i+50]), 0), model))\n",
    "# #     return resAll[51:]\n",
    "\n",
    "def genPath(seed, model):\n",
    "    path_w_seed = seed\n",
    "#     for i in range(target_length - len(seed)):\n",
    "    for i in range(60):\n",
    "#         print (len(torch.Tensor(path_w_seed[i:i + 50])))\n",
    "#         print(predict(torch.Tensor(path_w_seed[i:i + 50]).unsqueeze(1), model).shape)\n",
    "        path_w_seed = np.concatenate((path_w_seed, predict(torch.Tensor(path_w_seed[i:i + 50]).unsqueeze(0), model)))\n",
    "    return path_w_seed\n",
    "\n",
    "\n",
    "# print (np.array(pathPred).shape)\n",
    "\n",
    "# parse a path and separate the x and y coordinates\n",
    "def parsePath(path):\n",
    "    tempX = []\n",
    "    tempY = []\n",
    "    for elem in path:\n",
    "        tempX.append(elem[0])\n",
    "        tempY.append(elem[1])\n",
    "    return tempX, tempY\n",
    "\n",
    "\n",
    "# normalized, scale_2 = normalize(all_in_1)\n",
    "# \n",
    "# pathPred = genPath((all_in_1_WV[1800])[0:50], lstm)\n",
    "print (predict(torch.Tensor((all_in_1_WV[1800])[0:50]).unsqueeze(0), lstm))\n",
    "# print (\"true: \", normalized[1800][51])\n",
    "# scatX, scatY = parsePath(pathPred)\n",
    "# plt.scatter(scatX, scatY)\n",
    "trueX, trueY = parsePath(normalized[1800])\n",
    "# plt.scatter(trueX, trueY)\n",
    "\n",
    "# plt.show\n",
    "\n",
    "# print (torch.Tensor(all_in_1[0][0:50]).unsqueeze(1))\n",
    "# predict(torch.FloatTensor(all_in_1[0][0:50]), lstm)\n",
    "\n",
    "# print(scatX[49:])\n",
    "# print(trueX[49:])\n",
    "\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(222)\n",
    "\n",
    "# ax1.scatter(scatX, scatY, c='b', marker=\"o\", label='scat')\n",
    "ax2.scatter(trueX,trueY, c='r', marker=\"o\", label='true')\n",
    "plt.legend(loc='upper left');\n",
    "\n",
    "# ax = plt.gca()\n",
    "# ax.scatter(scatX, scatY, color=\"b\")\n",
    "# ax.scatter(trueX, trueY, color=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f80b5e4",
   "metadata": {},
   "source": [
    "## Sample a batch of data and visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6507c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 50, 4]) torch.Size([60, 2])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "def show_sample_batch(sample_batch):\n",
    "    \"\"\"visualize the trajectory for a batch of samples\"\"\"\n",
    "    inp, out = sample_batch\n",
    "    batch_sz = inp.size(0)\n",
    "    agent_sz = inp.size(1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,batch_sz, figsize=(15, 3), facecolor='w', edgecolor='k')\n",
    "    fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "    axs = axs.ravel()   \n",
    "    for i in range(batch_sz):\n",
    "        axs[i].xaxis.set_ticks([])\n",
    "        axs[i].yaxis.set_ticks([])\n",
    "        \n",
    "        # first two feature dimensions are (x,y) positions\n",
    "        axs[i].scatter(inp[i,:,0], inp[i,:,1])\n",
    "        axs[i].scatter(out[i,:,0], out[i,:,1])\n",
    "        \n",
    "# show_sample_batch(sample_batch)\n",
    "\n",
    "\n",
    "        \n",
    "for i_batch, sample_batch in enumerate(train_loader):\n",
    "    inp, out = sample_batch\n",
    "    print(inp.shape, out.shape)\n",
    "#     show_sample_batch(sample_batch)\n",
    "    break\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "      implement your Deep learning model\n",
    "      implement training routine\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00333419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for elem in y:\n",
    "y_show = y[2]\n",
    "    x_res = []\n",
    "    y_res = []\n",
    "    for i in range(0,120,2):\n",
    "        x_res.append(y_show[i])\n",
    "        y_res.append(y_show[i+1])\n",
    "    plt.scatter(x_res, y_res)\n",
    "    plt.show\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
