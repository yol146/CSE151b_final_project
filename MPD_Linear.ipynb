{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2212c26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    number of trajectories in each city\\n    # austin --  train: 43041 test: 6325 \\n    # miami -- train: 55029 test:7971\\n    # pittsburgh -- train: 43544 test: 6361\\n    # dearborn -- train: 24465 test: 3671\\n    # washington-dc -- train: 25744 test: 3829\\n    # palo-alto -- train:  11993 test:1686\\n\\n    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\\n    \\n'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, os.path \n",
    "import numpy \n",
    "import pickle\n",
    "from glob import glob\n",
    "# Get the device\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    number of trajectories in each city\n",
    "    # austin --  train: 43041 test: 6325 \n",
    "    # miami -- train: 55029 test:7971\n",
    "    # pittsburgh -- train: 43544 test: 6361\n",
    "    # dearborn -- train: 24465 test: 3671\n",
    "    # washington-dc -- train: 25744 test: 3829\n",
    "    # palo-alto -- train:  11993 test:1686\n",
    "\n",
    "    trajectories sampled at 10HZ rate, input 5 seconds, output 6 seconds\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80749d71",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "563cf07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ROOT_PATH = \"./argo2/\"\n",
    "\n",
    "cities = [\"austin\", \"miami\", \"pittsburgh\", \"dearborn\", \"washington-dc\", \"palo-alto\"]\n",
    "splits = [\"train\", \"test\"]\n",
    "\n",
    "# # intialize a dataset\n",
    "city = 'austin' \n",
    "split = 'train'\n",
    "\n",
    "def normalize(trajectories):\n",
    "    minxy = np.min(trajectories, axis=-1)\n",
    "    maxxy = np.max(trajectories, axis=-1)\n",
    "    scale = np.max(maxxy - minxy) / 2.0\n",
    "    return (trajectories / scale), scale\n",
    "\n",
    "def unnormalize(trajectories, scale):\n",
    "    return trajectories * scale\n",
    "\n",
    "\n",
    "def get_city_trajectories(city=\"palo-alto\", split=\"train\", normalized=False):\n",
    "    \n",
    "    outputs = None\n",
    "    \n",
    "    if split==\"train\":\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[:int(n * 0.8)]\n",
    "        \n",
    "        f_out = ROOT_PATH + split + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[:int(n * 0.8)]\n",
    "        \n",
    "    elif split == 'val':\n",
    "        f_in = ROOT_PATH + 'train' + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)[int(n * 0.8):]\n",
    "        \n",
    "        f_out = ROOT_PATH + 'train' + \"/\" + city + \"_outputs\"\n",
    "        outputs = pickle.load(open(f_out, \"rb\"))\n",
    "        outputs = np.asarray(outputs)[int(n * 0.8):]\n",
    "    \n",
    "    else:\n",
    "        f_in = ROOT_PATH + split + \"/\" + city + \"_inputs\"\n",
    "        inputs = pickle.load(open(f_in, \"rb\"))\n",
    "        n = len(inputs)\n",
    "        inputs = np.asarray(inputs)\n",
    "\n",
    "    return inputs, outputs\n",
    "\n",
    "class ArgoverseDataset(Dataset):\n",
    "    \"\"\"Dataset class for Argoverse\"\"\"\n",
    "    def __init__(self, city: str, split:str, transform=None):\n",
    "        super(ArgoverseDataset, self).__init__()\n",
    "        self.transform = transform\n",
    "\n",
    "        self.inputs, self.outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "#         tensor = torch.Tensor(self.feature[item])\n",
    "#         tensorlabel = torch.Tensor(self.label[item])\n",
    "#         return tensor.to(device), tensorlabel.to(device)\n",
    "\n",
    "        data = np.concatenate((self.inputs[idx], self.outputs[idx]))\n",
    "            \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return torch.Tensor(data).to(device)\n",
    "    \n",
    "# every path has length 110, with the 110nd character as target value\n",
    "# set range step higher if overfit     \n",
    "# if dataArr is with velocity, remove for y labels\n",
    "def create_set(dataArr, normalization=False):\n",
    "    normArr = dataArr\n",
    "    scale = 0\n",
    "    set_x = []\n",
    "    set_y = []\n",
    "    if(normalization):\n",
    "        normArr, scale = normalize(dataArr)\n",
    "    for path in normArr:\n",
    "            set_x.append(path[:109])\n",
    "            set_y.append(path[109])\n",
    "    return set_x, set_y, scale\n",
    "\n",
    "class mydataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \n",
    "        self.feature = x\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        tensor = torch.Tensor(self.feature[item])\n",
    "        tensorlabel = torch.Tensor(self.label[item])\n",
    "        return tensor.to(device), tensorlabel.to(device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature)\n",
    "    \n",
    "# input: n*110*2 paths\n",
    "# output: n*110*4 paths with dim2 = [x,y,Vx,Vy], where Vx, Vy are velocity with length/.1s as unit\n",
    "def addVelocity(dataArr):\n",
    "    dataWV = []\n",
    "    for path in np.array(dataArr):\n",
    "        pathWV = []\n",
    "        pathWV.append(np.concatenate((path[0], [0,0])))\n",
    "        for coord in range(1, len(path)):\n",
    "            vel = path[coord] - path[coord-1]\n",
    "            coordWV = np.concatenate((path[coord], vel))\n",
    "            pathWV.append(coordWV)\n",
    "        dataWV.append(pathWV)\n",
    "    return dataWV\n",
    "\n",
    "def calcVelocity(dataArr):\n",
    "    dataWV = []\n",
    "    for path in np.array(dataArr):\n",
    "        pathWV = []\n",
    "        pathWV.append(path[1] - path[0])\n",
    "        for coord in range(1, len(path)):\n",
    "            vel = path[coord] - path[coord-1]\n",
    "            pathWV.append(vel)\n",
    "        dataWV.append(pathWV)\n",
    "    return np.array(dataWV)\n",
    "\n",
    "# shifts every path to be starting from 0,0\n",
    "# returns shifted data Array, each array's shifted value\n",
    "def toOrigin (dataArr):\n",
    "    dataO = []\n",
    "    shiftArr = []\n",
    "    for path in np.array(dataArr):\n",
    "        pathO = []\n",
    "        shifter = path[0]\n",
    "        shiftArr.append(shifter)\n",
    "        for coord in range(len(path)):\n",
    "            shifted = path[coord] - shifter\n",
    "            pathO.append(shifted)\n",
    "        dataO.append(pathO)\n",
    "    return dataO, shiftArr\n",
    "\n",
    "# tempNorm, tempScale = normalize(all_in_1)\n",
    "# tempunNorm = unnormalize(tempNorm, tempScale)\n",
    "# print (\"tempNorm\", tempNorm)\n",
    "# print (\"unNorm\", tempunNorm)\n",
    "# print (\"original\", np.array(all_in_1))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# training_set_x, training_set_y, scale = create_set(all_in_1_WV)\n",
    "# train_dataset = mydataset(training_set_x, training_set_y)\n",
    "\n",
    "# velArr = np.array(calcVelocity(all_in_1))\n",
    "# all_in_1_O, shiftVel = toOrigin(all_in_1)\n",
    "# # define training set with velocity + shifted to origin\n",
    "# all_in_1_WV = addVelocity(all_in_1_O)\n",
    "# training_set_x, training_set_y, scale = create_set(all_in_1_WV, normalization=False)\n",
    "# train_dataset = mydataset(training_set_x, training_set_y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "53cb0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(training_set_y).shape)\n",
    "# print(training_set_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d4d0386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get train data\n",
    "inputs, outputs = get_city_trajectories(city=city, split=split, normalized=False)\n",
    "all_in_1 = []\n",
    "for i in range(len(inputs)):\n",
    "    togetherData = np.concatenate((inputs[i], outputs[i]))\n",
    "    all_in_1.append(togetherData)\n",
    "\n",
    "\n",
    "# Get Validation Data\n",
    "inputs, outputs = get_city_trajectories(city=city, split='val', normalized=False)\n",
    "val_in_1 = []\n",
    "for i in range(len(inputs)):\n",
    "    togetherData = np.concatenate((inputs[i], outputs[i]))\n",
    "    val_in_1.append(togetherData)\n",
    "    \n",
    "# random subsampling from val_set to make faster validation\n",
    "numValPath = 300\n",
    "val_index = np.random.choice(len(val_in_1), numValPath, replace=False)\n",
    "val_in_1_500 = np.array(val_in_1)[val_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "30dbd6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# takes in a path and return x,y lists\n",
    "def parsePath(path):\n",
    "    tempX = []\n",
    "    tempY = []\n",
    "    for elem in path:\n",
    "        tempX.append(elem[0])\n",
    "        tempY.append(elem[1])\n",
    "    return tempX, tempY\n",
    "\n",
    "def showPath(x,y):\n",
    "    plt.scatter(x[0:50],y[0:50], c='b')\n",
    "    plt.scatter(x[50:],y[50:], c='r')\n",
    "\n",
    "# gets the angle to rotate to pointing (1,1) in radians\n",
    "# the path needs to be shifted to (0,0) in the beginning\n",
    "# path should only have (x,y), i.e. without velocity\n",
    "def getAngle(path):\n",
    "#     shifted path starts at (0,0), so path[1] is its initial facing direction\n",
    "    direction = path[1]\n",
    "#     target direction\n",
    "    targetVec = np.array([1,1])\n",
    "    x1 = path[1][0]\n",
    "    x2 = 1\n",
    "    y1 = path[1][1]\n",
    "    y2 = 1\n",
    "        #     magnitude of direction vector for angle calculation, sqrt(2) is targetVec's magnitude\n",
    "    dot = x1*x2 + y1*y2      # dot product\n",
    "    det = x1*y2 - y1*x2      # determinant\n",
    "    angle = math.atan2(det, dot)  # atan2(y, x) or atan2(sin, cos)\n",
    "    return angle\n",
    "    \n",
    "    \n",
    "#   rotate the x,y with angle (as vector)\n",
    "# returns rotated x,y\n",
    "def rotatePoint(x, y, rad):\n",
    "    xR = x*math.cos(rad) - y*math.sin(rad)\n",
    "    yR = y*math.cos(rad) + x*math.sin(rad)\n",
    "    return xR, yR\n",
    "\n",
    "# rotate the paths in the data to initialize to the same direction\n",
    "# takes in the data and logArr for converting result back (logArr should contain the shift value)\n",
    "# returns rotated datapath, new logarr with angles corresponding to each path to rotate back (construct original long/lat)\n",
    "def rotateData(data, logArr):\n",
    "#     logArr should have same lenth with data, since each path is shifted to (0,0)\n",
    "    newLogArr = []\n",
    "    dataR = []\n",
    "    for i,path in enumerate(data):\n",
    "        pathR = []\n",
    "        ang = getAngle(path)\n",
    "        newLogArr.append(np.concatenate((logArr[i], [ang])))\n",
    "#         for each point, rotate them to math the orientation(except (0,0))\n",
    "        pathR.append(path[0])\n",
    "        for j in range(1, len(path)):\n",
    "            xp = path[j][0]\n",
    "            yp = path[j][1]\n",
    "#             get rotated point\n",
    "            xR, yR = rotatePoint(xp, yp, ang)\n",
    "            pathR.append([xR,yR])\n",
    "        dataR.append(pathR)\n",
    "    return dataR, newLogArr\n",
    "\n",
    "# shift & rotate data, takes in a raw data array (arbitrary length of paths, but each path has (x,y) points)\n",
    "# returns processed data (without velocity added, but shifted to origin & rotated)\n",
    "def shiftAndRotate(data):\n",
    "#     shift data, get shifted array & shifted values for converting back\n",
    "    data_O, shiftLog = toOrigin(data)\n",
    "#     get rotated data & logs for converting back\n",
    "    data_RO, logArr = rotateData(data_O, shiftLog)\n",
    "    return np.array(data_RO), logArr\n",
    "\n",
    "# takes in raw dataset and prep(shift to origin -> unify starting direction -> add velocity -> return proessedDataSet, convBackArr)\n",
    "# the normalized = false since we already shifted data to origin.\n",
    "def prepDataSet(data, normalized=False):\n",
    "    # shift & rotate data\n",
    "    processedData, convBackArr = shiftAndRotate(data)\n",
    "    \n",
    "    # now we have shifted & rotated data, then add velocity\n",
    "#     Testing without velocity: TODO: If bad, put it back!!!==========================================\n",
    "    data_WV = addVelocity(processedData)\n",
    "    set_x, set_y, scale = create_set(data_WV, normalization=normalized)\n",
    "    \n",
    "    processedDataSet = mydataset(set_x, set_y)\n",
    "    return processedDataSet, convBackArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "84bc029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, trainConvBack = prepDataSet(all_in_1)\n",
    "val_set, valConvBack = prepDataSet(val_in_1_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7446a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f0909ce",
   "metadata": {},
   "source": [
    "Data Processing Done (shift to origin -> unify starting direction -> add velocity -> return proessedDataSet, convBackArr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfffa02",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "83515dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 64  # batch size \n",
    "train_loader = DataLoader(train_set,batch_size=batch_sz, shuffle = True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_sz, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f082417",
   "metadata": {},
   "source": [
    "# LSTM Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "32d92e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        #Hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        #Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        #imput dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        \n",
    "        #Building your LSTM\n",
    "        #batch_first=True causes input/output tensors to be of shape\n",
    "        #(batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        #Readout layer\n",
    "        self.fc = nn.Linear (hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        #Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        \n",
    "        #Output channel\n",
    "        output, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        hn = hn.view(-1,self.hidden_dim)\n",
    "        out = self.fc(hn)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc26525",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "ddbb4666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0 \n",
      " LSTMModel(\n",
      "  (lstm): LSTM(4, 250, batch_first=True)\n",
      "  (fc): Linear(in_features=250, out_features=4, bias=True)\n",
      ")\n",
      "lstm.weight_ih_l0 torch.Size([1000, 4])\n",
      "lstm.weight_hh_l0 torch.Size([1000, 250])\n",
      "lstm.bias_ih_l0 torch.Size([1000])\n",
      "lstm.bias_hh_l0 torch.Size([1000])\n",
      "fc.weight torch.Size([4, 250])\n",
      "fc.bias torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "# define hyper parameters\n",
    "learning_rate = 0.0005\n",
    "epochs = 50\n",
    "input_dim = 4\n",
    "hidden_dim = 250\n",
    "layer_count = 1\n",
    "output_dim = 4\n",
    "\n",
    "lstm = LSTMModel(input_dim, hidden_dim, layer_count, output_dim).to(device)\n",
    "print(device, \"\\n\", lstm)\n",
    "\n",
    "# loss = nn.CrossEntropyLoss()\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# visualize the inner data shape\n",
    "for name, param in lstm.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4fa4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
